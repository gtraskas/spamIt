{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam or Ham?\n",
    "\n",
    "Implement a spam filter in Python using the Naive Bayes algorithm to classify the emails as spam or not-spam (a.k.a. ham).\n",
    "\n",
    "## Check Modules\n",
    "\n",
    "Check your system for the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for nltk...\n",
      "\n",
      "Checking for numpy...\n",
      "\n",
      "Checking for pandas...\n",
      "\n",
      "Checking for scipy...\n",
      "\n",
      "Checking for sklearn...\n",
      "\n",
      "Checking for pickle...\n",
      "\n",
      "Checking for re...\n",
      "\n",
      "System is ready!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "dependencies = [\"nltk\", \"numpy\", \"pandas\", \"scipy\", \"sklearn\", \"pickle\", \"re\"]\n",
    "\n",
    "for module in dependencies:\n",
    "    print(\"\\nChecking for \" + module + \"...\")\n",
    "    try:\n",
    "        # Import module from string variable:\n",
    "        # https://stackoverflow.com/questions/8718885/import-module-from-string-variable\n",
    "        # To import using a variable, call __import__(name)\n",
    "        module_obj = __import__(module)\n",
    "        # To contain the module, create a global object using globals()\n",
    "        globals()[module] = module_obj\n",
    "    except ImportError:\n",
    "        print(\"Install \" + module + \" before continuing\")\n",
    "        print(\"In a terminal type the following commands:\")\n",
    "        print(\"python get-pip.py\")\n",
    "        print(\"pip install \" + module + \"\\n\")\n",
    "        sys.exit(1)\n",
    "\n",
    "print(\"\\nSystem is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset\n",
    "\n",
    "Download a set of spam and ham actual emails. Each email is a separate plain text file. Unzip the compressed tar files, read the text and load it into a Pandas Dataframe. Convert the dataframe to a Pickle object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Enron emails in the Downloads folder...\n",
      "Download, unzip, and save to pickle done!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import tarfile\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Downloading Enron emails in the Downloads folder...\")\n",
    "\n",
    "# Get the user's Downloads folder path\n",
    "downloads = os.path.join(os.environ['HOME'] + \"/Downloads\")\n",
    "\n",
    "url = \"http://www.aueb.gr/users/ion/data/enron-spam/preprocessed/\"\n",
    "\n",
    "enron_dir = os.path.join(downloads, 'Enron emails')\n",
    "\n",
    "enron_files = ['enron1.tar.gz', 'enron2.tar.gz', 'enron3.tar.gz',\n",
    "               'enron4.tar.gz', 'enron5.tar.gz', 'enron6.tar.gz']\n",
    "\n",
    "def download():\n",
    "    \"\"\" Download Enron emails if missing. \"\"\"\n",
    "    \n",
    "    # Create the directories.\n",
    "    if not os.path.exists(enron_dir):\n",
    "        os.makedirs(enron_dir)\n",
    "    # Download the files that not exist.\n",
    "    for file in enron_files:\n",
    "        path = os.path.join(enron_dir, file)\n",
    "        if not os.path.exists(path):\n",
    "            urllib.request.urlretrieve(url + file, path)\n",
    "\n",
    "def extract_emails(fname):\n",
    "    \"\"\" Extract the zipped emails and load them into a pandas df.\n",
    "    Args:\n",
    "        fname (str): the files with tar.gz extension\n",
    "    Returns:\n",
    "        pandas df: a pandas dataframe of emails\n",
    "    \"\"\"\n",
    "    \n",
    "    rows = []\n",
    "    tfile = tarfile.open(fname, 'r:gz')\n",
    "    for member in tfile.getmembers():\n",
    "        if 'ham' in member.name:\n",
    "            f = tfile.extractfile(member)\n",
    "            if f is not None:\n",
    "                row = f.read()\n",
    "                rows.append({'message': row, 'class': 'ham'})\n",
    "        if 'spam' in member.name:\n",
    "            f = tfile.extractfile(member)\n",
    "            if f is not None:\n",
    "                row = f.read()\n",
    "                rows.append({'message': row, 'class': 'spam'})\n",
    "    tfile.close()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def populate_df_and_pickle():\n",
    "    \"\"\" Populate the df with all the emails and save it to a pickle object. \"\"\"\n",
    "    \n",
    "    if not os.path.exists(downloads + \"/emails.pickle\"):\n",
    "        emails_df = pd.DataFrame({'message': [], 'class': []})\n",
    "        for file in enron_files:\n",
    "            unzipped_file = extract_emails(os.path.join(enron_dir, file))\n",
    "            emails_df = emails_df.append(unzipped_file)\n",
    "        emails_df.to_pickle(downloads + \"/emails.pickle\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    download()\n",
    "    populate_df_and_pickle()\n",
    "    print(\"Download, unzip, and save to pickle done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=33716, step=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(33716, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(downloads + '/emails.pickle', 'rb') as f:\n",
    "    emails_df = pickle.load(f) \n",
    "\n",
    "# Translate bytes objects into strings.\n",
    "emails_df['message'] = emails_df['message'].apply(lambda x: x.decode('latin-1'))\n",
    "\n",
    "# Reset pandas df index.\n",
    "emails_df = emails_df.reset_index(drop=True)\n",
    "\n",
    "# Map 'spam' to 1 and 'ham' to 0.\n",
    "emails_df['class'] = emails_df['class'].map({'spam':1, 'ham':0})\n",
    "\n",
    "print(emails_df.index)\n",
    "emails_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1,\n",
       "       \"Subject: [ ilug - social ] prirodu requiremus social sample\\r\\nsocial\\r\\non january lst 2002 , the european countries began\\r\\nusing the new euro . never before have so\\r\\nmany countries with such powerful economies united\\r\\nto use a single currency . get your piece of history\\r\\nnow ! we would like to send you a free euro\\r\\nand a free report on world currency . just visit\\r\\nour site to request your euro and euro report :\\r\\nin addition to our currency report , you can receive\\r\\nour free investment package :\\r\\n* learn how $ 10 , 000 in options will leverage $ 1 , 000 , 000 in\\r\\neuro currency . this means even a small movement in the market\\r\\nhas huge profit potential . csice\\r\\nif you are over age 18 and have some risk capital , it ' s\\r\\nimportant that you find out how the euro will\\r\\nchange the economic world and how you can profit !\\r\\nplease carefully evaluate your financial position before\\r\\ntrading . only risk capital should be used .\\r\\n8 c 43 fd 25 cb 6 f 949944 eel 2 c 379 e 50028\\r\\nutbxcuhepuffbnkwq\\r\\nfull opt - out instructions on the bottom of the site\\r\\n- -\\r\\nirish linux users ' group social events : social @ linux . ie\\r\\nhttp : / / www . linux . ie / mailman / listinfo / social for ( un ) subscription information .\\r\\nlist maintainer : listmaster @ linux . ie\"], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails_df.iloc[25000].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Data\n",
    "\n",
    "Remove the punctuation, any urls and numbers. Finally, convert every word to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1,\n",
       "       'subject  ilug  social  prirodu requiremus social sample\\r social\\r on january lst    the european countries began\\r using the new euro  never before have so\\r many countries with such powerful economies united\\r to use a single currency  get your piece of history\\r now  we would like to send you a free euro\\r and a free report on world currency  just visit\\r our site to request your euro and euro report \\r in addition to our currency report  you can receive\\r our free investment package \\r  learn how       in options will leverage          in\\r euro currency  this means even a small movement in the market\\r has huge profit potential  csice\\r if you are over age   and have some risk capital  it  s\\r important that you find out how the euro will\\r change the economic world and how you can profit \\r please carefully evaluate your financial position before\\r trading  only risk capital should be used \\r   c   fd   cb   f   eel   c   e  \\r utbxcuhepuffbnkwq\\r full opt  out instructions on the bottom of the site\\r  \\r irish linux users  group social events  social  linux  ie\\r http    www  linux  ie  mailman  listinfo  social for  un  subscription information \\r list maintainer  listmaster  linux  ie'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "def clean_email(email):\n",
    "    \"\"\" Remove all punctuation, urls, numbers, and newlines.\n",
    "    Convert to lower case.\n",
    "    Args:\n",
    "        email (unicode): the email\n",
    "    Returns:\n",
    "        email (unicode): only the text of the email\n",
    "    \"\"\"\n",
    "    \n",
    "    email = re.sub(r'http\\S+', ' ', email)\n",
    "    email = re.sub(\"\\d+\", \" \", email)\n",
    "    email = email.replace('\\n', ' ')\n",
    "    email = email.translate(str.maketrans(\"\", \"\", punctuation))\n",
    "    email = email.lower()\n",
    "    return email\n",
    "\n",
    "emails_df['message'] = emails_df['message'].apply(clean_email)\n",
    "\n",
    "emails_df.iloc[25000].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data\n",
    "\n",
    "Split the text string into individual words and stem each word. Remove english stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and Stem\n",
    "\n",
    "Split the text by white spaces and link the different forms of the same word to each other, using stemming. For example \"responsiveness\" and \"response\" have the same stem/root - \"respons\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stop Words\n",
    "\n",
    "Some words such as “the” or “is” appear in all emails and don’t have much content to them. These words are not going to help the algorithm distinguish spam from ham. Such words are called stopwords and they can be disregarded during classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1,\n",
       "       'subject ilug social prirodu requiremus social sampl social on januari lst the european countri began use the new euro never befor have so mani countri with such power economi unit to use a singl currenc get your piec of histori now we would like to send you a free euro and a free report on world currenc just visit our site to request your euro and euro report in addit to our currenc report you can receiv our free invest packag learn how in option will leverag in euro currenc this mean even a small movement in the market has huge profit potenti csice if you are over age and have some risk capit it s import that you find out how the euro will chang the econom world and how you can profit pleas care evalu your financi posit befor trade onli risk capit should be use c fd cb f eel c e utbxcuhepuffbnkwq full opt out instruct on the bottom of the site irish linux user group social event social linux ie http www linux ie mailman listinfo social for un subscript inform list maintain listmast linux ie '], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# nltk.download('wordnet') # uncomment to download 'wordnet'\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def preproces_text(email):\n",
    "    \"\"\" Split the text string into individual words, stem each word,\n",
    "    and append the stemmed word to words. Make sure there's a single\n",
    "    space between each stemmed word.\n",
    "    Args:\n",
    "        email (unicode): the email\n",
    "    Returns:\n",
    "        words (unicode): the text of the email\n",
    "    \"\"\"\n",
    "    \n",
    "    words = \"\"\n",
    "    # Create the stemmer.\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    # Split text into words.\n",
    "    email = email.split()\n",
    "    for word in email:\n",
    "        # Optional: remove unknown words.\n",
    "        # if wn.synsets(word):\n",
    "        words = words + stemmer.stem(word) + \" \"\n",
    "    \n",
    "    return words\n",
    "\n",
    "emails_df['message'] = emails_df['message'].apply(preproces_text)\n",
    "\n",
    "emails_df.iloc[25000].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "### Vectorize Words and Split Data to Train/Test Sets\n",
    "\n",
    "Transform the words into a tf-idf matrix using the sklearn TfIdf transformation. Then, create train/test sets with the `train_test_split` function, using `stratify` parameter. The dataset is highly unbalanced and the `stratify` parameter will make a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter `stratify`. For example, if variable y is 0 and 1 and there are 30% of 0's and 70% of 1's, `stratify=y` will make sure that the random split has 30% of 0's and 75% of 1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different words: 119405\n",
      "Word example: arcadian\n",
      "(26972, 119405) (26972,)\n",
      "(6744, 119405) (6744,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the independent variables as Xs.\n",
    "Xs = emails_df['message'].values\n",
    "\n",
    "# Define the target (dependent) variable as Ys.\n",
    "Ys = emails_df['class'].values\n",
    "\n",
    "# Vectorize words - Turn the text numerical feature vectors,\n",
    "# using the strategy of tokenization, counting and normalization.\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                                       stop_words='english')\n",
    "Xs = vectorizer.fit_transform(Xs)\n",
    "\n",
    "# Create a train/test split using 20% test size.\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs,\n",
    "                                                    Ys,\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=Ys)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "print(\"Number of different words: {0}\".format(len(feature_names)))\n",
    "print(\"Word example: {0}\".format(feature_names[5369]))\n",
    "\n",
    "# Check the split printing the shape of each set.\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Classifier\n",
    "\n",
    "Train a Naive Bayes classifier and evaluate the performance with the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9847271648873073\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create classifier.\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Fit the classifier on the training features and labels.\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction - Store predictions in a list named pred.\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy on the test data.\n",
    "print(\"Accuracy: {}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the Most Powerful Features\n",
    "\n",
    "Print the 10 most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.10193040638 money\n",
      "-7.08106062291 price\n",
      "-7.07724882029 onlin\n",
      "-7.07696063312 offer\n",
      "-7.06439782381 www\n",
      "-7.04630242466 softwar\n",
      "-6.97568091654 email\n",
      "-6.94140085524 click\n",
      "-6.65836580587 com\n",
      "-6.59068342497 http\n"
     ]
    }
   ],
   "source": [
    "def get_most_important_features(vectorizer, classifier, n=None):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    top_features = sorted(zip(classifier.coef_[0], feature_names))[-n:]\n",
    "    for coef, feat in top_features:\n",
    "        print(coef, feat)\n",
    "\n",
    "get_most_important_features(vectorizer, clf, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "Let's try out our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = [\"Hello George, how about a game of tennis tomorrow?\",\n",
    "         \"Hello, click here if you want to satisfy your wife tonight\",\n",
    "         \"We offer free viagra!!! Click here now!!!\",\n",
    "         \"Dear Sara, I prepared the annual report. Please check the attachment.\",\n",
    "         \"Hi David, will we go for cinema tonight?\",\n",
    "         \"Best holidays offers only here!!!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = vectorizer.transform(email)\n",
    "predictions = clf.predict(examples)\n",
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
